{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch\n",
    "\n",
    "**Nicolas Nytko**  \n",
    "nnytko2@illinois.edu  \n",
    "https://github.com/nicknytko/siam-pytorch-workshop\n",
    "\n",
    "Adapted from materials by Matthew West (mwest@illinois.edu)\n",
    "\n",
    "_SIAM @ Illinois workshop series_\n",
    "\n",
    "November 12, 2024\n",
    "\n",
    "**Description:** PyTorch allows you to easily train and run machine learning models. It uses standard Python methods for writing code, so it's both simple and powerful. We will cover the core automatic differentiation capabilities of PyTorch, training deep neural networks, managing training and test data, saving and loading models, and show a few examples of neural network implementations. We will assume a good knowledge of Python and NumPy, and basic knowledge of machine learning with neural nets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List of resources\n",
    "\n",
    "- PyTorch tutorials: https://pytorch.org/tutorials/\n",
    "- PyTorch manual: https://pytorch.org/docs/stable/index.html\n",
    "- PyTorch paper: https://openreview.net/forum?id=BJJsrmfCZ\n",
    "- Calculus in Machine Learning: https://machinelearningmastery.com/calculus-in-machine-learning-why-it-works/\n",
    "- Calculus on computational graphs: http://colah.github.io/posts/2015-08-Backprop/\n",
    "- Einstein summation in PyTorch: https://rockt.github.io/2018/04/30/einsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First, some math background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three main \"approaches\" to machine learning:\n",
    "- Supervised learning\n",
    "- Unsupervised learning\n",
    "- Reinforcement learning\n",
    "\n",
    "We'll focus on supervised learning, though what we'll show here will be applicable to all 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we have some unknown function $f : \\mathbb{R}^m \\to \\mathbb{R}^n$; for example, a function that takes an image and outputs $1$ if it contains a cat, and $0$ otherwise.\n",
    "\n",
    "![](figures/f_catdog.png)\n",
    "\n",
    "<small> Dog photo from https://www.pickpik.com/dog-puppy-cute-adorable-pet-cute-puppy-59501 </small>\n",
    "\n",
    "This is pretty easy for humans to determine!  However, we don't know what such a function \"looks like\" nor how we would implement it on a computer.  So, we seek an approximate function $\\hat{f} \\approx f$ where $\\hat{f}$ is much easier for us to evaluate.\n",
    "\n",
    "Here's a smaller example with a linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(0, 1, 50)\n",
    "f = 5 + 3*x + np.random.randn(50)*0.1\n",
    "\n",
    "plt.plot(x, f, 'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the output of $f$ is displayed for a few values of $x$.  This is pretty simple, but useful for demonstration!  Let us try to fit a $\\hat{f}$ that approximates $f$ closely.  Lets assume $\\hat{f}$ is a linear function of the form\n",
    "$$ \\hat{f}(x; m, b) = mx + b. $$\n",
    "One way of forcing $\\hat{f} \\approx f$ is to minimize the function\n",
    "$$ \\ell(m, b) = \\sum_i (f(x_i, m, b) - f(x))^2 = \\sum_i (mx_i + b - f(x))^2 $$\n",
    "How do we do this?  Recall from calculus that to find the minimum of a function, we can take the derivatives and solve for $0$.\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial \\ell}{\\partial m} &= \\sum_i 2x_i(mx_i + b - f(x)) \\\\\n",
    "\\frac{\\partial \\ell}{\\partial b} &= \\sum_i 2(mx_i + b - f(x))\n",
    "\\end{align*}$$\n",
    "\n",
    "Numerically, we can implement this using *gradient descent*.  We start with some random initial values of $m, b$ and update like\n",
    "$$\\begin{align*}\n",
    "m &\\gets m - \\alpha \\frac{\\partial \\ell}{\\partial m}(m, b), \\\\\n",
    "b &\\gets b - \\alpha \\frac{\\partial \\ell}{\\partial b}(m, b),\n",
    "\\end{align*}$$\n",
    "where $\\alpha \\in \\mathbb{R}^+$ is some small parameter called the *learning rate*.  Lets see it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with an arbitrary guess for the slope and bias\n",
    "m = 1.0\n",
    "b = 0.0\n",
    "\n",
    "alpha = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute derivatives\n",
    "dl_dm = np.sum(2 * x * (m * x + b - f))\n",
    "dl_db = np.sum(2 * (m * x + b - f))\n",
    "\n",
    "# Descend\n",
    "m -= dl_dm * alpha\n",
    "b -= dl_db * alpha\n",
    "\n",
    "# Plot the results\n",
    "f_hat = m * x + b\n",
    "\n",
    "plt.plot(x, f, 'o')\n",
    "plt.plot(x, f_hat)\n",
    "plt.ylim(np.min(f) - 1, np.max(f) + 1)\n",
    "\n",
    "print(f'Loss: {np.sum((f_hat - f) ** 2):.3f}')\n",
    "print(f'Derivatives: {dl_dm:.3e} {dl_db:.3e}')\n",
    "print(f'm: {m:.3f}, b: {b:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the basic idea behind supervised learning!  Of course, we did everything by hand, like taking the derivatives and performing gradient descent, which is a lot of work as our models will get more complex.  As we'll see, PyTorch can implement much of this for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's get started with PyTorch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.linalg as tla\n",
    "import random, datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor([1.0, 2.0, 3.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cos(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tla.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify datatypes\n",
    "\n",
    "Use `dtype=torch.float64` and `.double()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x.double()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.log(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1, 2, 3], dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.log(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Annoying differences from NumPy\n",
    "\n",
    "`np.sum(x, axis=1)` versus `torch.sum(x, dim=1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = torch.Tensor([[1,2,3], [4,5,6]])\n",
    "Tary = np.array([[1,2,3], [4,5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(Tary, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(T, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to/from NumPy arrays\n",
    "\n",
    "`.numpy()` and `torch.from_numpy()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "y = torch.from_numpy(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory is shared!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0] = 7\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `torch.autograd`: Computing derivatives\n",
    "\n",
    "PyTorch constructs the computation graph as you do operations (dynamic graphs) unlike TensorFlow (static graphs)\n",
    "\n",
    "Using the computation graph, the chain rule (back propagation) can compute derivatives\n",
    "\n",
    "Derivatives are available in the leaf nodes\n",
    "\n",
    "![](figures/computation_graph.jpg)\n",
    "\n",
    "<small>Figure from http://datahacker.rs/004-computational-graph-and-autograd-with-pytorch/</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor(3.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = x * y**2\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'x.grad = {x.grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z = x y^2$\n",
    "\n",
    "$\\frac{\\partial z}{\\partial y} = 2 x y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2*x*y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Control what we differentiate with respect to\n",
    "\n",
    "`requires_grad=True`\n",
    "\n",
    "`with no_grad():`\n",
    "\n",
    "`.detach()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x*x\n",
    "print(f'y.requires_grad = {y.requires_grad}')\n",
    "z = x*y\n",
    "z.backward()\n",
    "print(f'dz/dx = {x.grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x*x\n",
    "y = y.detach() # can't say y.requires_grad = False\n",
    "print(f'y.requires_grad = {y.requires_grad}')\n",
    "z = x*y\n",
    "z.backward()\n",
    "print(f'dz/dx = {x.grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "with torch.no_grad():\n",
    "    y = x*x\n",
    "print(f'y.requires_grad = {y.requires_grad}')\n",
    "z = x*y\n",
    "z.backward()\n",
    "print(f'dz/dx = {x.grad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computation graphs are not trees\n",
    "\n",
    "Re-using a parameter in multiple places makes the graph not be a tree. It's a DAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = 3*x\n",
    "z = x**2\n",
    "w = y + z + x\n",
    "w.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial w}{\\partial x} = \\frac{\\partial}{\\partial x}(3x + x^2 + x) = 3 + 2x + 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3 + 2*x + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The computation graph is destroyed by `backward()`\n",
    "\n",
    "To retain it for more differentiation, use `backward(retain_graph=True)`\n",
    "\n",
    "A common use case is multiple outputs with a shared subgraph\n",
    "\n",
    "Don't forget to free the graph on the last call to prevent memory leaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "y = x**2\n",
    "z1 = 3*y\n",
    "z2 = 4*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1.backward() # (retain_graph=True)\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z2.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derivatives of scalars with respect to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = (x**2).sum()\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Don't do in-place modifications to tensors\n",
    "\n",
    "But it's fine to do `x = 4 * x`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[1] = x[2] + 1\n",
    "#x = 4*x\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = (x**2).sum()\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results can be slightly different from what you expect...\n",
    "\n",
    "Since we're building the graph as computations are being done, functions like `max()` become differentiable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.0, 2.0, 4.0, 3.0, 0.5], requires_grad=True)\n",
    "max_x = torch.max(x)\n",
    "max_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_x.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `torch.optim`: All the common gradient-based optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvec = np.linspace(-2, 2, 100)\n",
    "fvec = f(xvec)\n",
    "plt.plot(xvec, fvec, 'o-', markersize=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([2.0], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.SGD([x], lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_history = [x.detach().numpy().copy()]\n",
    "for i in range(30):\n",
    "    print(f'##########')\n",
    "    print(f'i = {i}')\n",
    "    print(f'initial x = {x}')\n",
    "    opt.zero_grad()\n",
    "    z = f(x)\n",
    "    print(f'f(x) = {z}')\n",
    "    z.backward()\n",
    "    print(f'x.grad = {x.grad}')\n",
    "    opt.step()\n",
    "    print(f'updated x = {x}')\n",
    "    x_history.append(x.detach().numpy().copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvec = np.linspace(-2, 2, 100)\n",
    "fvec = f(xvec)\n",
    "plt.plot(xvec, fvec)\n",
    "plt.plot(x_history, f(np.array(x_history)), 'o-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `torch.nn`: Lets implement a neural network!\n",
    "\n",
    "Recall from the first example, we implemented a linear model to approximate some noisy data.\n",
    "\n",
    "What happens if we compose multiple linear models together? \n",
    "\n",
    "$$\\begin{align*}\n",
    "f_1{\\bf x} &= {\\bf W}_1 {\\bf x} + {\\bf b}_1 \\\\\n",
    "f_2{\\bf x} &= {\\bf W}_2 {\\bf x} + {\\bf b}_2 \\\\\n",
    "\\hat{f}({\\bf x}) &= f_2(f_1({\\bf x})) = {\\bf W}_2({\\bf W}_1 {\\bf x} + {\\bf b}_1) + {\\bf b}_2\n",
    "\\end{align*}$$\n",
    "\n",
    "If we distribute terms out, we get something that's still linear in ${\\bf x}$!  \n",
    "$$ \\hat{f}({\\bf x}) = {\\bf W}_2({\\bf W}_1 {\\bf x} + {\\bf b}_1) + {\\bf b}_2 = {\\bf W}_2 {\\bf W}_1 {\\bf x} + {\\bf W}_2 {\\bf b}_1 + {\\bf b}_2 $$\n",
    "\n",
    "This means that ultimately stacking many linear models together does not give us something more expressive.  However, we can introduce a nonlinearity in terms of an *activation function*.  There are many choices, like\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{ReLU}({\\bf x})_i &= \\max\\{x_i, 0\\} \\\\\n",
    "\\sigma({\\bf x})_i &= e^x/(1 + e^x) \\\\\n",
    "\\text{Softplus}({\\bf x})_i &= \\ln(1+e^x) \\\\\n",
    "&\\enspace\\vdots\n",
    "\\end{align*}$$\n",
    "\n",
    "and many more.  We'll stick to ReLU here as it's pretty simple.  Adding these nonlinear functions to our model, we get\n",
    "$$\\begin{align*}\n",
    "f_1{\\bf x} &= \\text{ReLU}({\\bf W}_1 {\\bf x} + {\\bf b}_1) \\\\\n",
    "f_2{\\bf x} &= \\text{ReLU}({\\bf W}_2 {\\bf x} + {\\bf b}_2) \\\\\n",
    "\\hat{f}({\\bf x}) &= f_2(f_1({\\bf x})) = \\text{ReLU}({\\bf W}_2\\text{ReLU}({\\bf W}_1 {\\bf x} + {\\bf b}_1) + {\\bf b}_2)\n",
    "\\end{align*}$$\n",
    "Notice this is no longer linear in ${\\bf x}$.  Lets go ahead and implement this in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(0, 2*np.pi, 100)\n",
    "y = torch.sin(x)\n",
    "plt.plot(x.numpy(), y.numpy(), 'r.');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        N = 8\n",
    "        self.fc1 = torch.nn.Linear(1, N)\n",
    "        self.fc2 = torch.nn.Linear(N, N)\n",
    "        self.fc3 = torch.nn.Linear(N, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape((-1, 1)) # We'll reshape this to a column vector\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = torch.nn.functional.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yp = model(x)\n",
    "\n",
    "plt.plot(x.numpy(), y.numpy(), 'r.');\n",
    "plt.plot(x.numpy(), yp.detach().numpy());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc3.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc3.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc3.bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(200):\n",
    "    opt.zero_grad()\n",
    "    \n",
    "    yp = model(x)\n",
    "    loss = torch.nn.MSELoss()(yp, y)\n",
    "    \n",
    "    loss_history.append(loss.item())\n",
    "    loss.backward()\n",
    "    \n",
    "    opt.step()\n",
    "\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('Training iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x.numpy(), y.numpy(), 'r.');\n",
    "plt.plot(x.numpy(), yp.detach().numpy());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and restoring models\n",
    "\n",
    "Save and load the parameters, not the full models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_file.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel()\n",
    "model.load_state_dict(torch.load('model_file.pth', weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
