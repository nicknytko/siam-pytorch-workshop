{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cbcd26-5414-449c-a5cd-c5ea7d488dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.linalg as tla\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63b4034-1fb5-46c4-b342-69899ff44d21",
   "metadata": {},
   "source": [
    "We want to simulate the PDE\n",
    "$$ \\frac{\\partial u}{\\partial t} + u \\frac{\\partial u}{\\partial x} = 0 $$\n",
    "on the domain $\\Omega = [-1, 1]$, with initial and boundary conditions\n",
    "$$\\begin{align*}\n",
    "u(x, 0) &= -\\sin(\\pi x) \\\\\n",
    "u(-1, t) &= 0 \\\\\n",
    "u(1, t) &= 0 \\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "For context, here is what the solution is supposed to look like: https://www.youtube.com/watch?v=bDNXNGpYj0c.\n",
    "\n",
    "One way we can solve this PDE this is to *learn* the function $\\hat{u}(x, t)$ that will approximate the solution, where\n",
    "$$ \\hat{u} : \\mathbb{R}^2 \\to \\mathbb{R} $$\n",
    "is a function parameterized by a feed-forward neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57438de9-1665-440d-8220-2155124da061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will learn the function from t=0 to t=0.75\n",
    "t0 = 0\n",
    "t1 = 0.75\n",
    "\n",
    "# Initial condition in time\n",
    "def u0(x):\n",
    "    return -torch.sin(torch.pi * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d24567c-8b17-4812-9bb8-762109588a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(-1, 1, 50)\n",
    "plt.plot(x, u0(x), '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cbc3a7-234a-46a4-8b74-4cdd35f7d26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FCN, self).__init__()\n",
    "        N = 16\n",
    "        self.fc_layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2, N), torch.nn.Tanh(),\n",
    "            torch.nn.Linear(N, N), torch.nn.Tanh(),\n",
    "            torch.nn.Linear(N, N), torch.nn.Tanh(),\n",
    "            torch.nn.Linear(N, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, u, t):\n",
    "        return self.fc_layers(torch.column_stack((u, t))).flatten()\n",
    "\n",
    "def plot_solution(u_hat, t0, t1, grid_spacing=128, time_steps=50):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    x = torch.linspace(-1, 1, grid_spacing)\n",
    "    t = torch.linspace(t0, t1, time_steps)\n",
    "    line, = ax.plot(x, u_hat(x, torch.ones_like(x) * t0).detach())\n",
    "\n",
    "    def init():\n",
    "        return line,\n",
    "\n",
    "    def update(t_):\n",
    "        line.set_data(x, u_hat(x, torch.ones_like(x) * t_).detach())\n",
    "        return line,\n",
    "\n",
    "    anim = FuncAnimation(fig, update, frames=t, init_func=init, blit=True, interval=40)\n",
    "    plt.close(fig)\n",
    "    return HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc95274-95b5-450a-9750-43ee6760824c",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_hat = FCN()\n",
    "plot_solution(u_hat, t0, t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3ad5a8-be3e-4771-81ff-8383f8814cde",
   "metadata": {},
   "source": [
    "Nothing interesting happens, but we haven't trained anything!  First, lets learn the initial condition by just optimizing $$\\ell = \\|\\hat{u}(x, 0) - u(x, 0)\\|$$ over some discrete samples $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66437577-96d1-408b-92fe-d38d4ff3369c",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.zeros_like(x)\n",
    "opt = torch.optim.Adam(u_hat.parameters(), lr=0.005)\n",
    "\n",
    "for i in range(1_000):\n",
    "    opt.zero_grad()\n",
    "    ell = tla.norm(u_hat(x,t).flatten() - u0(x))\n",
    "    ell.backward()\n",
    "    opt.step()\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        print(i, ell.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a91a089-7113-4294-85ed-c940ff3da04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_solution(u_hat, t0, t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2227b5-ffba-4d21-8221-ac8efa73a7c2",
   "metadata": {},
   "source": [
    "That works, but notice how the boundary conditions are only respected at $t=0$.  Lets modify our loss function to penalize the boundary terms, giving us\n",
    "$$\\ell = \\|\\hat{u}(x, 0) - u(x, 0)\\| + \\|\\hat{u}(-1, t)\\| + \\|\\hat{u}(1, t)\\|,$$\n",
    "over now some discrete $x$, $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116846db-f17e-4014-a87f-9a005b28f6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# space, time for initial conditions\n",
    "x_ic = torch.linspace(-1, 1, 50)\n",
    "t_ic = torch.zeros_like(x)\n",
    "\n",
    "# space, time for boundary conditions\n",
    "t_bc = torch.linspace(t0, t1, 50)\n",
    "x_l_bc = torch.ones_like(t_bc) * -1\n",
    "x_r_bc = torch.ones_like(t_bc) * 1\n",
    "\n",
    "opt = torch.optim.Adam(u_hat.parameters(), lr=0.001)\n",
    "\n",
    "for i in range(1_000):\n",
    "    # fill me\n",
    "    opt.zero_grad()\n",
    "    ell_ic = tla.norm(u_hat(x_ic, t_ic) - u0(x))\n",
    "    ell_bc_l = ...\n",
    "    ell_bc_r = ...\n",
    "    ell = ell_ic + ell_bc_l + ell_bc_r\n",
    "    ell.backward()\n",
    "    opt.step()\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        print(i, ell.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93bdd05-a7e9-4550-b910-14880332a2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_solution(u_hat, t0, t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8af5df-89cc-45de-a55d-fe8010b02960",
   "metadata": {},
   "source": [
    "Now for the interesting part!  We still haven't satisfied the derivatives of the PDE! How do we do this without expensive derivative approximations?  Well, we can use PyTorch's autograd to compute the derivatives for us.  For our reference, here is the formulation of the PDE again:\n",
    "$$ \\frac{\\partial u}{\\partial t} +u \\frac{\\partial u}{\\partial x} = 0 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd4b42c-c47c-4eb6-a2cb-97c0c806dce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_linear = torch.linspace(-1, 1, 52)[1:-1]\n",
    "t_linear = torch.linspace(t0, t1, 51)[1:]\n",
    "\n",
    "x_pde, t_pde = torch.meshgrid(x_linear, t_linear, indexing='ij')\n",
    "x_pde = x_pde.flatten()\n",
    "t_pde = t_pde.flatten()\n",
    "x_pde.requires_grad = True\n",
    "t_pde.requires_grad = True\n",
    "\n",
    "# what does the above do? for each time step, it creates a spatial grid we can evaluate our function at.\n",
    "plt.scatter(x_pde.detach(), t_pde.detach(), s=1.0)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca475a0-d6b1-49ad-b7bf-81f6cb8df48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_eval = u_hat(x_pde, t_pde)\n",
    "\n",
    "du_dx = torch.autograd.grad(u_eval, x_pde, \n",
    "                            grad_outputs=torch.ones_like(x_pde), # Shape of the output\n",
    "                            create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "du_dt = torch.autograd.grad(u_eval, t_pde, \n",
    "                            grad_outputs=torch.ones_like(x_pde), # Shape of the output\n",
    "                            create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "print(du_dx)\n",
    "print(du_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df470af7-d306-4f1d-a4a0-24be59436e37",
   "metadata": {},
   "source": [
    "How well do we currently satisfy the PDE portion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f294f27-2dab-4b80-9fbe-67f5e2b00bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean((du_dt + u_eval * du_dx)**2.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389e4e9f-e2af-4995-8d04-ae60046da5d8",
   "metadata": {},
   "source": [
    "Lets combine everything into one large loss function and train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dcf9fc-cb25-4edc-a2a7-7b792a54cc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# space, time for initial conditions\n",
    "x_ic = torch.linspace(-1, 1, 50)\n",
    "t_ic = torch.zeros_like(x)\n",
    "\n",
    "# space, time for boundary conditions\n",
    "t_bc = torch.linspace(t0, t1, 50)\n",
    "x_l_bc = torch.ones_like(t_bc) * -1\n",
    "x_r_bc = torch.ones_like(t_bc) * 1\n",
    "\n",
    "# space, time for pde residual\n",
    "x_linear = torch.linspace(-1, 1, 52)[1:-1]\n",
    "t_linear = torch.linspace(t0, t1, 20)[1:]\n",
    "\n",
    "x_pde, t_pde = torch.meshgrid(x_linear, t_linear, indexing='ij')\n",
    "x_pde = x_pde.flatten()\n",
    "t_pde = t_pde.flatten()\n",
    "x_pde.requires_grad = True\n",
    "t_pde.requires_grad = True\n",
    "\n",
    "opt = torch.optim.Adam(u_hat.parameters(), lr=1e-3)\n",
    "\n",
    "for i in range(50_000):\n",
    "    # combine everything and fill out this loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10f9a33-ce03-4f56-8d5e-26d85de30bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_solution(u_hat, t0, t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a3f5a4-f141-40d6-8cda-04e927f65611",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
